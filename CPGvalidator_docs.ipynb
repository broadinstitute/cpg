{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOALS\n",
    ">DESCRIBE:\n",
    "What is the input\n",
    "What is the output\n",
    "\n",
    ">Overview—: to the readme\n",
    "Include, what is a bucket! --> done!(ish)\n",
    "\n",
    ">Rules: \n",
    "How to construct new rules\n",
    "How to run them\n",
    "Hoe to understand the output\n",
    "\n",
    ">In progress is a valid flag bc this work is being done\n",
    "\n",
    ">Inventory autogenerates ¿once a week? --> Yes! but the index is generated manually\n",
    "\n",
    ">Explain the example case for downloading files\n",
    "\n",
    ">README describing what it does\n",
    "Tutorial and or examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "`cpgdata` is a CLI toolset for navigating and exploring the Cell Painting Gallery.\n",
    "\n",
    "\n",
    "## Context\n",
    "\n",
    "The CellPainting Gallery (CPG) is hosted in a cloud object storage, AWS's S3.\n",
    "\n",
    "Within this system, all files within the CPG live in a single 'bucket'. Because it is an object storage, this bucket does not contain a folder structure, but rather all objects live together. In order to identify them, each object is assigned a unique 'key' that consists of a string of characters, akin to the directory path in a regular file structure with folders.\n",
    "\n",
    "For example, <'s3://cellpainting-gallery/cpg0016-jump/source_4/workspace/analysis/2021_06_21_Batch7/BR00125168/analysis/BR00125168-G17-5/Cells.csv'> is the key to a specific .csv file that lives within the CPG S3 bucket.\n",
    "\n",
    "The [**aws inventory**]( https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html) is a list of all the objects contained in a bucket and their associated metadata (i.e. object size, date of upload, last modified date, etc). This inventory is updated automatically on a weekly basis for the CPG.\n",
    "\n",
    "However, the format in which this inventory is structured is not friendly for exploring it. \n",
    "\n",
    "That is why, we use `cpgdata` to parse (using `cpgparser`, a Python library written in Rust) the inventory, retireving and organizing useful information about all objects in a dataframe called the **Index**. You can [manually browse the Index contents online using Quilt](https://open.quiltdata.com/b/cellpainting-gallery/tree/).\n",
    "\n",
    "The `cpgdata` package also provides tools to navigate and filter the Index file which can then be used to selectively download certain files from the CPG or explore its contents.\n",
    "\n",
    "Moreover, these same tools can also be used to create rules to validate the structure and completeness of new data before uploading it to the CPG bucket, to ensure that it complies with the CPG requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Inventory** for the CPG lives in <s3://cellpainting-gallery-inventory/cellpainting-gallery/whole_bucket/>\n",
    "\n",
    "s3://cellpainting-gallery-inventory/\n",
    "    └── cellpainting-gallery/\n",
    "        └──index/\n",
    "            └── [all the index chunks in .parquet format]\n",
    "        └── whole_bucket/\n",
    "            ├──2024-03-31T01-00Z/\n",
    "            ├── 2024-04-07T01-00Z/\n",
    "            ├── data/\n",
    "            └──hive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Index** file lives in <s3://cellpainting-gallery/index.html>\n",
    "\n",
    "If you want to get an idea of the expected file structure of the CPG you do so [HERE](https://broadinstitute.github.io/cellpainting-gallery/data_structure.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In its current version, `cpgdata` runs on Python 3.10 so the first step will be to create an environment to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create --name cpgdata python==3.10\n",
    "!conda activate cpgdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, you need to install the <cpgparser> and <cpgdata> packages (only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cpgparser\n",
    "!pip install cpgdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# We use polars to read and explore the index \n",
    "import polars as pl\n",
    "from cpgdata.utils import parallel, download_s3_files\n",
    "\n",
    "#These were included in the example but not necessary for the code so far\n",
    "# from typing import List\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "# import os\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generation of the **Index** file is a very time-consuming process, but you don't need to do it yourself! \n",
    "### You can easily download it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a local diretory in which to download the Index\n",
    "\n",
    "index_dir = Path(\"/Users/emigliet/Documents/CPGvalidator_docs\")\n",
    "# index_dir = Path(\"Your/Local/Destination/Directory\")\n",
    "\n",
    "# Note that the Index file is fairly large (over 20gb) so it's divided into several .parquet files.\n",
    "# !cpg sync index {index_dir}\n",
    "\n",
    "# Load the index using polars (pl)\n",
    "index_files = [file for file in index_dir.glob(\"*.parquet\")]\n",
    "index = pl.scan_parquet(index_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns included in the Index file:\n",
    "\n",
    "- `key` : object key identifier, useful for downloading files\n",
    "- `root_dir`: ## if col(\"worskpace')==\"workspace, from 'workspace' to leaf node, \n",
    "- `images_root_dir`: path after \"{dataset_id}/{source_id}/{batch_id}/images/\" to the object. Is 'null' is object is not within that path.\n",
    "- `images_batch_root_dir`:  path after \"{dataset_id}/{source_id}/{batch_id}/\" to the object. Is 'null' if object is no within that path.\n",
    "- `images_illum_root_dir`: path after \"{dataset_id}/{source_id}/{batch_id}/images/illum/\" to the object. If \"is_dir\"==True, \"images_illum_root_dir\"==\"plate_id\". If \"is_dir\"==False, \"images_illum_root_dir\"==\"plate_id/illumFile\".\n",
    "- `images_images_root_dir`: path after \"{dataset_id}/{source_id}/{batch_id}/images/{plate_id}/images/\" to the object. Is 'null' if object is no within that path. \n",
    "- `images_images_aligned_root_dir`: \n",
    "- `images_images_corrected_root_dir`: \n",
    "- `images_images_corrected_cropped_root_dir`: \n",
    "- `workspace_root_dir`: path after \"{dataset_id}/{source_id}/workspace/\" to the object. Is 'null' is object is not within that path.\n",
    "- `analysis_root_dir`: \n",
    "- `backend_root_dir`: \n",
    "- `load_data_csv_root_dir`: path from \"load_data_csv\" to leaf node (?)\n",
    "- `metadata_root_dir`: \n",
    "- `profiles_root_dir`: \n",
    "- `assaydev_root_dir`: \n",
    "- `embeddings_root_dir`: \n",
    "- `pipelines_root_dir`: \n",
    "- `qc_root_dir`: \n",
    "- `segmentation_root_dir`: \n",
    "- `software_root_dir`: \n",
    "- `workspace_dl_root_dir`: \n",
    "- `collated_root_dir`: \n",
    "- `consensus_root_dir`: \n",
    "- `dl_embeddings_root_dir`: \n",
    "- `dl_profiles_root_dir`: \n",
    "- `sep`: \n",
    "- `images`: is \"images\" if \"images\" is part of the key. Is \"null\" otherwise.\n",
    "- `workspace`: is \"workspace\" if \"workspace\" is part of the key. Is \"null\" otherwise.\n",
    "- `workspace_dl`: \n",
    "- `dataset_id`: name of the dataset (e.g. \"cpg0016-jump\", \"cpg0021-periscope\", etc.)\n",
    "- `source_id`: code for the source of the images (the institution who produced them)\n",
    "- `batch_id`: batch number\n",
    "- `plate_id`:  unique plate identification code\n",
    "- `well_id`: well position\n",
    "- `site_id`: site number (sites are each of the fields of view imaged in a well)\n",
    "- `well_site_id`: \n",
    "- `plate_well_site_id`: \n",
    "- `ml_model_id`: \n",
    "- `leaf_node`: if the object is a file (\"is_dir\"==False), the name of the file. Otherwise, 'null'.\n",
    "- `filename`: leaf node filename, without extension\n",
    "- `extension`: leaf node extension\n",
    "- `software_hash`: \n",
    "- `software`: \n",
    "- `hash`: \n",
    "- `allowed_names`: \n",
    "- `bucket`: \n",
    "- `obj_key`: \n",
    "- `size`: \n",
    "- `last_modified_date`: \n",
    "- `e_tag`: \n",
    "- `storage_class`: \n",
    "- `is_multipart_uploaded`: \n",
    "- `replication_status`: \n",
    "- `encryption_status`: \n",
    "- `object_lock_retain_until_date`: \n",
    "- `object_lock_mode`: \n",
    "- `object_lock_legal_hold_status`: \n",
    "- `intelligent_tiering_access_tier`: \n",
    "- `bucket_key_status`: \n",
    "- `checksum_algorithm`: \n",
    "- `object_access_control_list`: \n",
    "- `object_owner`: \n",
    "- `is_parsing_error`: \n",
    "- `errors`: \n",
    "- `is_dir`: \n",
    "- `key_parts`: \n",
    "- `workspace_dir`: if within \"workspace\", which workspace dir is object related to ('profiles', 'load_data_csv', 'software', 'metadata', 'backend', 'quality_control', 'assaydev', 'analysis', 'pipelines'). Is 'None' if the object does not contain \"workspace\" in it's key or is in  \n",
    " \n",
    "You can see all the Index column names and the type of data stored in each using `df.schema`.\n",
    "\n",
    "**Refer to CPG schema and use those same keys! check matching and come up with useful key maybe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of index col names and their respective data types\n",
    "index.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: use the index to download just a specific subset of files from the CPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **Polars** tools in the following examples to explore and filter the index.\n",
    "You can find more info on the context and expressions in:\n",
    "https://docs.pola.rs/user-guide/concepts/contexts/\n",
    "https://docs.pola.rs/user-guide/concepts/expressions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the maximum length of displayed strings to 500 helps to visualize the complete keys\n",
    "pl.Config(fmt_str_lengths=500)\n",
    "pl.Config.set_fmt_table_cell_list_len(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull the keys (file location within the bucket) of all 'Cells.csv' files from Source 4 in the JUMP dataset (cpg0016-jump)\n",
    "\n",
    "df = (\n",
    "    index\n",
    "    #Use filtering to get to the failing rows and not the other way around\n",
    "    .filter(pl.col(\"dataset_id\").eq(\"cpg0016-jump\"))  \n",
    "    .filter(pl.col(\"source_id\").eq(\"source_4\"))\n",
    "    .filter(pl.col(\"leaf_node\").str.contains(\"Cells.csv\"))\n",
    "     \n",
    "     # Always add a `select` at the end of the chain and ONLY select for keys\n",
    "    .select(pl.col(\"key\",\"load_data_csv_root_dir\"))   \n",
    "    \n",
    "    # Materialize this polars LazyFrame into a DataFrame.\n",
    "    .collect(streaming=True)  # the streaming option prevents out of memory errors when loading big dataframes\n",
    ")\n",
    "\n",
    "# print first 10 results\n",
    "print(df.to_dicts()[0:10])\n",
    "\n",
    "# List the keys of the files to download\n",
    "download_keys = list(df.to_dict()[\"key\"])\n",
    "\n",
    "# Choose a destination directory for the files\n",
    "dest_dir = \"Path/To/Save/Your/files\"\n",
    "\n",
    "# Run a parallel command to dowload all files specified in the list of keys\n",
    "parallel(download_keys, download_s3_files, [\"cellpainting-gallery\", Path(dest_dir)], jobs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Tests and issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some useful structures for filtering and selecting\n",
    "\n",
    "    # .filter(pl.col(\"images_images_root_dir\").is_in([\"2020_11_04_CPJUMP1\", \"2020_11_19_TimepointDay4\", \"2020_12_08_CPJUMP1_Bleaching\"])) \n",
    "    # .filter(pl.col(\"leaf_node\").str.contains(\"^.*(.tiff)\"))\n",
    "    # .filter(pl.col(\"well_id\").eq(\"E7\")) \n",
    "\n",
    "    # .select(pl.col(\"well_id\").unique())\n",
    "    # .select(pl.col(\"*\").exclude([\"size\", \"is_multipart_uploaded\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### There appear to be some issues when parsing well_id, particularly in the embedding.parquet files from sources 1, 2 and 7 of the JUMP dataset.\n",
    "\n",
    "df = (\n",
    "    index\n",
    "    .unique(subset=\"well_id\")\n",
    "    .filter(pl.col(\"is_parsing_error\").eq(False)) \n",
    "    .select(\"well_id\", \"key\", \"dataset_id\", \"source_id\", \"leaf_node\")\n",
    "    .unique(subset=[\"dataset_id\",\"leaf_node\",\"source_id\"])\n",
    "    .collect(streaming=True)\n",
    "    )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: write this validations:\n",
    " - Is there a folder with illum files for every plate within raw images?\n",
    " - Is there a load_data.csv for every plate, is there a load_data csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea: compare total number of unique plates in raw images with number of unique load.csv files and illum folders\n",
    "\n",
    "Works fine fo just source 4 of JUMP but fails when applied bucket-wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total number of distinct plates within the raw images folder\n",
    "df1 = (\n",
    "    index\n",
    "    .filter(pl.col(\"dataset_id\").eq(\"cpg0016-jump\"))\n",
    "    .filter(pl.col(\"source_id\").eq(\"source_4\"))\n",
    "    \n",
    "    .filter(pl.col(\"is_dir\").eq(True))\n",
    "    .filter(pl.col(\"images\").eq(\"images\"))\n",
    "    .filter(pl.col(\"images_images_root_dir\").is_not_null())\n",
    "    .filter(pl.col(\"dataset_id\").eq(\"jump\").not_()) #this dataset_id has parsing errors\n",
    "    .select(\"key\", \"dataset_id\", \"source_id\", \"batch_id\", \"plate_id\")\n",
    "    .unique(subset=[\"dataset_id\", \"source_id\", \"batch_id\", \"plate_id\"]) # gives me 1150 unique plates\n",
    "    # .unique(subset=[\"plate_id\"]) # gives me 1150 unique plates\n",
    "    .collect(streaming=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# get total number of load_data.csv files\n",
    "df2 = (\n",
    "    index\n",
    "    .filter(pl.col(\"dataset_id\").eq(\"cpg0016-jump\"))\n",
    "    .filter(pl.col(\"source_id\").eq(\"source_4\"))\n",
    "    \n",
    "    .filter(pl.col(\"workspace\").eq(\"workspace\"))\n",
    "    .filter(pl.col(\"leaf_node\").eq(\"load_data.csv\"))\n",
    "    .filter(pl.col(\"dataset_id\").eq(\"jump\").not_()) #this dataset_id has parsing errors\n",
    "    .select(\"key\", \"dataset_id\", \"source_id\", \"batch_id\", \"plate_id\")\n",
    "    # .unique(subset=[\"dataset_id\", \"source_id\", \"batch_id\", \"plate_id\"]) #gives me 3478 unique combinations\n",
    "    .unique(subset=[\"plate_id\"]) # gives me 3652 unique plates if I don't filter the \"jump\" dataset_id because of parsing shenanigans\n",
    "    .collect(streaming=True)\n",
    "    )\n",
    "\n",
    "# get total number of illum/ folders\n",
    "df3 = (\n",
    "    index\n",
    "    .filter(pl.col(\"dataset_id\").eq(\"cpg0016-jump\"))\n",
    "    .filter(pl.col(\"source_id\").eq(\"source_4\"))\n",
    "    \n",
    "    .filter(pl.col(\"is_dir\").eq(True))\n",
    "    .filter(pl.col(\"images\").eq(\"images\"))\n",
    "    .filter(pl.col(\"images_illum_root_dir\").is_not_null())\n",
    "    .filter(pl.col(\"dataset_id\").eq(\"jump\").not_()) #this dataset_id has parsing errors\n",
    "    .select(\"key\", \"dataset_id\", \"source_id\", \"batch_id\", \"plate_id\")\n",
    "    .unique(subset=[\"dataset_id\", \"source_id\", \"batch_id\", \"plate_id\"]) # gives me 2386 unique plates\n",
    "    # .unique(subset=[\"plate_id\"]) # gives me 2386 unique plates\n",
    "    .collect(streaming=True)\n",
    "    )\n",
    "\n",
    "print(f\"df1: {df1.shape}\")\n",
    "print(f\"df2: {df2.shape}\")\n",
    "print(f\"df3: {df3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev-cpg0016-jump, deflaux-workflow-test-BR00117012: number of plates and load_data.csv files MATCH\n",
      "dev-cpg0016-jump, deflaux-workflow-test-BR00117012: number of plates and illum folders MATCH\n",
      "cpg0011-lipocyteprofiler, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0011-lipocyteprofiler, broad: number of plates and illum folders MATCH\n",
      "cpg0025-dactyloscopy, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0025-dactyloscopy, broad: number of plates and illum folders MATCH\n",
      "dev-cpg0016-jump, deflaux-workflow-test2: number of plates and load_data.csv files MATCH\n",
      "dev-cpg0016-jump, deflaux-workflow-test2: number of plates and illum folders MATCH\n",
      "cpg0015-heterogeneity, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0015-heterogeneity, broad: number of plates and illum folders MATCH\n",
      "cpg0016-jump-fixed, source_4: number of plates and load_data.csv files MATCH\n",
      "cpg0016-jump-fixed, source_4: number of plates and illum folders MATCH\n",
      "test-cpg0016-jump, source_4: number of plates and load_data.csv files MATCH\n",
      "test-cpg0016-jump, source_4: number of plates and illum folders MATCH\n",
      "cpg0016-jump, source_5: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0016-jump, source_5: --ERROR-- number of plates and illum folders DON'T MATCH\n",
      "cpg0023-mpi, mpi: number of plates and load_data.csv files MATCH\n",
      "cpg0023-mpi, mpi: number of plates and illum folders MATCH\n",
      "cpg0000-jump-pilot, source_4: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0000-jump-pilot, source_4: number of plates and illum folders MATCH\n",
      "cpg0016-jump, source_9: number of plates and load_data.csv files MATCH\n",
      "cpg0016-jump, source_9: number of plates and illum folders MATCH\n",
      "cpg0016-jump, source_10: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0016-jump, source_10: --ERROR-- number of plates and illum folders DON'T MATCH\n",
      "cpg0004-lincs, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0004-lincs, broad: number of plates and illum folders MATCH\n",
      "cpg0031-caicedo-cmvip, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0031-caicedo-cmvip, broad: number of plates and illum folders MATCH\n",
      "cpg0018-singh-seedseq, x: number of plates and load_data.csv files MATCH\n",
      "cpg0018-singh-seedseq, x: number of plates and illum folders MATCH\n",
      "cpg0003-rosetta, None: number of plates and load_data.csv files MATCH\n",
      "cpg0003-rosetta, None: number of plates and illum folders MATCH\n",
      "cpg0016-jump, source_6: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0016-jump, source_6: --ERROR-- number of plates and illum folders DON'T MATCH\n",
      "cpg0006-miami, broad: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0006-miami, broad: number of plates and illum folders MATCH\n",
      "cpg0016-jump, source_1: number of plates and load_data.csv files MATCH\n",
      "cpg0016-jump, source_1: --ERROR-- number of plates and illum folders DON'T MATCH\n",
      "dev-cpg0016-jump, source_4: number of plates and load_data.csv files MATCH\n",
      "dev-cpg0016-jump, source_4: number of plates and illum folders MATCH\n",
      "cpg0016-jump-fixed, source_7: number of plates and load_data.csv files MATCH\n",
      "cpg0016-jump-fixed, source_7: number of plates and illum folders MATCH\n",
      "cpg0001-cellpainting-protocol, source_4: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0001-cellpainting-protocol, source_4: number of plates and illum folders MATCH\n",
      "cpg0016-jump, source_4: number of plates and load_data.csv files MATCH\n",
      "cpg0016-jump, source_4: number of plates and illum folders MATCH\n",
      "cpg0016-jump, source_3: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0016-jump, source_3: --ERROR-- number of plates and illum folders DON'T MATCH\n",
      "cpg0002-jump-scope, source_4: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0002-jump-scope, source_4: --ERROR-- number of plates and illum folders DON'T MATCH\n",
      "cpg0016-jump, source_8: number of plates and load_data.csv files MATCH\n",
      "cpg0016-jump, source_8: number of plates and illum folders MATCH\n",
      "cpg0021-periscope, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0021-periscope, broad: number of plates and illum folders MATCH\n",
      "cpg0028-kelley-resistance, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0028-kelley-resistance, broad: number of plates and illum folders MATCH\n",
      "cpg0030-gustafsdottir-cellpainting, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0030-gustafsdottir-cellpainting, broad: number of plates and illum folders MATCH\n",
      "cpg0016-jump, source_2: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0016-jump, source_2: --ERROR-- number of plates and illum folders DON'T MATCH\n",
      "cpg0003-rosetta, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0003-rosetta, broad: number of plates and illum folders MATCH\n",
      "cpg0026-lacoste_haghighi-rare-diseases, broad: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0026-lacoste_haghighi-rare-diseases, broad: --ERROR-- number of plates and illum folders DON'T MATCH\n",
      "cpg0022-cmqtl, broad: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0022-cmqtl, broad: number of plates and illum folders MATCH\n",
      "cpg0017-rohban-pathways, broad: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0017-rohban-pathways, broad: number of plates and illum folders MATCH\n",
      "cpg0018-singh-seedseq, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0018-singh-seedseq, broad: number of plates and illum folders MATCH\n",
      "cpg0020-varchamp, broad: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0020-varchamp, broad: number of plates and illum folders MATCH\n",
      "cpg0009-molglue, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0009-molglue, broad: number of plates and illum folders MATCH\n",
      "cpg0024-bortezomib, source_4: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0024-bortezomib, source_4: number of plates and illum folders MATCH\n",
      "dev-cpg0016-jump, deflaux-workflow-tests: number of plates and load_data.csv files MATCH\n",
      "dev-cpg0016-jump, deflaux-workflow-tests: number of plates and illum folders MATCH\n",
      "cpg0010-caie-drugresponse, broad-az: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0010-caie-drugresponse, broad-az: number of plates and illum folders MATCH\n",
      "cpg0012-wawer-bioactivecompoundprofiling, broad: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0012-wawer-bioactivecompoundprofiling, broad: number of plates and illum folders MATCH\n",
      "cpg0016-jump-fixed, source_1: number of plates and load_data.csv files MATCH\n",
      "cpg0016-jump-fixed, source_1: number of plates and illum folders MATCH\n",
      "cpg0005-gerry-bioactivity, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0005-gerry-bioactivity, broad: number of plates and illum folders MATCH\n",
      "cpg0016-jump, source_11: number of plates and load_data.csv files MATCH\n",
      "cpg0016-jump, source_11: number of plates and illum folders MATCH\n",
      "cpg0019-moshkov-deepprofiler, broad: number of plates and load_data.csv files MATCH\n",
      "cpg0019-moshkov-deepprofiler, broad: number of plates and illum folders MATCH\n",
      "cpg0016-jump, source_13: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0016-jump, source_13: --ERROR-- number of plates and illum folders DON'T MATCH\n",
      "dev-cpg0016-jump, deflaux_test: number of plates and load_data.csv files MATCH\n",
      "dev-cpg0016-jump, deflaux_test: number of plates and illum folders MATCH\n",
      "jump, source_15: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "jump, source_15: number of plates and illum folders MATCH\n",
      "cpg0014-jump-adipocyte, broad: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0014-jump-adipocyte, broad: number of plates and illum folders MATCH\n",
      "dev-cpg0016-jump, deflaux-workflow-test-BR00125638-B22: number of plates and load_data.csv files MATCH\n",
      "dev-cpg0016-jump, deflaux-workflow-test-BR00125638-B22: number of plates and illum folders MATCH\n",
      "cpg0016-jump, source_7: --ERROR-- number of plates and load_data.csv files DON'T MATCH\n",
      "cpg0016-jump, source_7: --ERROR-- number of plates and illum folders DON'T MATCH\n"
     ]
    }
   ],
   "source": [
    "# Try to check within each dataset and source to find where there are missmatches between plates and load_data.csv or illum files\n",
    "\n",
    "df0 =(\n",
    "    index\n",
    "    .unique(subset=[\"dataset_id\",\"source_id\"])\n",
    "    .filter(pl.col(\"dataset_id\").is_not_null())\n",
    "    .select(pl.col(\"dataset_id\",\"source_id\"))\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "for dataset,source in zip(df0[\"dataset_id\"],df0[\"source_id\"]):\n",
    "    # get total number of distinct plates within the raw images folder\n",
    "    df1 = (\n",
    "        index\n",
    "        .filter(pl.col(\"dataset_id\").eq(dataset))\n",
    "        .filter(pl.col(\"source_id\").eq(source))\n",
    "        \n",
    "        .filter(pl.col(\"is_dir\").eq(True))\n",
    "        .filter(pl.col(\"images\").eq(\"images\"))\n",
    "        .filter(pl.col(\"images_images_root_dir\").is_not_null())\n",
    "        .select(\"key\", \"dataset_id\", \"source_id\", \"batch_id\", \"plate_id\")\n",
    "        .unique(subset=[\"plate_id\"])\n",
    "        .collect(streaming=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    # get total number of load_data.csv files\n",
    "    df2 = (\n",
    "        index\n",
    "        .filter(pl.col(\"dataset_id\").eq(dataset))\n",
    "        .filter(pl.col(\"source_id\").eq(source))\n",
    "        \n",
    "        .filter(pl.col(\"workspace\").eq(\"workspace\"))\n",
    "        .filter(pl.col(\"leaf_node\").eq(\"load_data.csv\"))\n",
    "        .select(\"key\", \"dataset_id\", \"source_id\", \"batch_id\", \"plate_id\")\n",
    "        .unique(subset=[\"plate_id\"]) \n",
    "        .collect(streaming=True)\n",
    "        )\n",
    "\n",
    "    # get total number of illum/ folders\n",
    "    df3 = (\n",
    "        index\n",
    "        .filter(pl.col(\"dataset_id\").eq(dataset))\n",
    "        .filter(pl.col(\"source_id\").eq(source))\n",
    "        \n",
    "        .filter(pl.col(\"is_dir\").eq(True))\n",
    "        .filter(pl.col(\"images\").eq(\"images\"))\n",
    "        .filter(pl.col(\"images_illum_root_dir\").is_not_null())\n",
    "        .select(\"key\", \"dataset_id\", \"source_id\", \"batch_id\", \"plate_id\")\n",
    "        .unique(subset=[\"plate_id\"]) \n",
    "        .collect(streaming=True)\n",
    "        )\n",
    "\n",
    "    if df1.shape[0]==df2.shape[0]:\n",
    "        print(f\"{dataset}, {source}: number of plates and load_data.csv files MATCH\")\n",
    "    else:\n",
    "        print(f\"{dataset}, {source}: --ERROR-- number of plates and load_data.csv files DON'T MATCH\")\n",
    "\n",
    "    if df1.shape[0]==df3.shape[0]:\n",
    "        print(f\"{dataset}, {source}: number of plates and illum folders MATCH\")\n",
    "    else:\n",
    "        print(f\"{dataset}, {source}: --ERROR-- number of plates and illum folders DON'T MATCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>well_id</th><th>key</th><th>dataset_id</th><th>source_id</th><th>leaf_node</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;UL001673&quot;</td><td>&quot;cpg0016-jump/source_1/workspace_dl/embeddings/efficientnet_v2_imagenet21k_s_feature_vector_2_0260bc96/Batch2_20221006/UL001673/UL001673/A02/embedding.parquet&quot;</td><td>&quot;cpg0016-jump&quot;</td><td>&quot;source_1&quot;</td><td>&quot;embedding.parquet&quot;</td></tr><tr><td>&quot;CP3-SC1-07&quot;</td><td>&quot;cpg0016-jump/source_7/workspace_dl/embeddings/efficientnet_v2_imagenet21k_s_feature_vector_2_0260bc96/20210727_Run3/CP3-SC1-07/CP3-SC1-07/A01/embedding.parquet&quot;</td><td>&quot;cpg0016-jump&quot;</td><td>&quot;source_7&quot;</td><td>&quot;embedding.parquet&quot;</td></tr><tr><td>&quot;M07&quot;</td><td>&quot;cpg0019-moshkov-deepprofiler/broad/workspace_dl/embeddings/105281_zenodo7114558/BBBC022/20585/M07/1/embedding.npz&quot;</td><td>&quot;cpg0019-moshkov-deepprofiler&quot;</td><td>&quot;broad&quot;</td><td>&quot;embedding.npz&quot;</td></tr><tr><td>null</td><td>&quot;cpg0016-jump/source_8/&quot;</td><td>&quot;cpg0016-jump&quot;</td><td>&quot;source_8&quot;</td><td>null</td></tr><tr><td>&quot;1086292259&quot;</td><td>&quot;cpg0016-jump/source_2/workspace_dl/embeddings/efficientnet_v2_imagenet21k_s_feature_vector_2_0260bc96/20210816_Batch_9/1086292259/1086292259/A01/embedding.parquet&quot;</td><td>&quot;cpg0016-jump&quot;</td><td>&quot;source_2&quot;</td><td>&quot;embedding.parquet&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌────────────┬──────────────────────────┬──────────────────────────┬───────────┬───────────────────┐\n",
       "│ well_id    ┆ key                      ┆ dataset_id               ┆ source_id ┆ leaf_node         │\n",
       "│ ---        ┆ ---                      ┆ ---                      ┆ ---       ┆ ---               │\n",
       "│ str        ┆ str                      ┆ str                      ┆ str       ┆ str               │\n",
       "╞════════════╪══════════════════════════╪══════════════════════════╪═══════════╪═══════════════════╡\n",
       "│ UL001673   ┆ cpg0016-jump/source_1/wo ┆ cpg0016-jump             ┆ source_1  ┆ embedding.parquet │\n",
       "│            ┆ rkspace_dl/embeddings/ef ┆                          ┆           ┆                   │\n",
       "│            ┆ ficientnet_v2_imagenet21 ┆                          ┆           ┆                   │\n",
       "│            ┆ k_s_feature_vector_2_026 ┆                          ┆           ┆                   │\n",
       "│            ┆ 0bc96/Batch2_20221006/UL ┆                          ┆           ┆                   │\n",
       "│            ┆ 001673/UL001673/A02/embe ┆                          ┆           ┆                   │\n",
       "│            ┆ dding.parquet            ┆                          ┆           ┆                   │\n",
       "│ CP3-SC1-07 ┆ cpg0016-jump/source_7/wo ┆ cpg0016-jump             ┆ source_7  ┆ embedding.parquet │\n",
       "│            ┆ rkspace_dl/embeddings/ef ┆                          ┆           ┆                   │\n",
       "│            ┆ ficientnet_v2_imagenet21 ┆                          ┆           ┆                   │\n",
       "│            ┆ k_s_feature_vector_2_026 ┆                          ┆           ┆                   │\n",
       "│            ┆ 0bc96/20210727_Run3/CP3- ┆                          ┆           ┆                   │\n",
       "│            ┆ SC1-07/CP3-SC1-07/A01/em ┆                          ┆           ┆                   │\n",
       "│            ┆ bedding.parquet          ┆                          ┆           ┆                   │\n",
       "│ M07        ┆ cpg0019-moshkov-deepprof ┆ cpg0019-moshkov-deepprof ┆ broad     ┆ embedding.npz     │\n",
       "│            ┆ iler/broad/workspace_dl/ ┆ iler                     ┆           ┆                   │\n",
       "│            ┆ embeddings/105281_zenodo ┆                          ┆           ┆                   │\n",
       "│            ┆ 7114558/BBBC022/20585/M0 ┆                          ┆           ┆                   │\n",
       "│            ┆ 7/1/embedding.npz        ┆                          ┆           ┆                   │\n",
       "│ null       ┆ cpg0016-jump/source_8/   ┆ cpg0016-jump             ┆ source_8  ┆ null              │\n",
       "│ 1086292259 ┆ cpg0016-jump/source_2/wo ┆ cpg0016-jump             ┆ source_2  ┆ embedding.parquet │\n",
       "│            ┆ rkspace_dl/embeddings/ef ┆                          ┆           ┆                   │\n",
       "│            ┆ ficientnet_v2_imagenet21 ┆                          ┆           ┆                   │\n",
       "│            ┆ k_s_feature_vector_2_026 ┆                          ┆           ┆                   │\n",
       "│            ┆ 0bc96/20210816_Batch_9/1 ┆                          ┆           ┆                   │\n",
       "│            ┆ 086292259/1086292259/A01 ┆                          ┆           ┆                   │\n",
       "│            ┆ /embedding.parquet       ┆                          ┆           ┆                   │\n",
       "└────────────┴──────────────────────────┴──────────────────────────┴───────────┴───────────────────┘"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There appear to be some issues when parsing well_id, particularly in the embedding.parquet files from sources 1, 2 and 7 of the JUMP dataset. \n",
    "# The well_id listed in the index corresponds to the previous \"segment\" of the key.\n",
    "\n",
    "df = (\n",
    "    index\n",
    "    .unique(subset=\"well_id\")\n",
    "    .filter(pl.col(\"is_parsing_error\").eq(False)) \n",
    "    .select(\"well_id\", \"key\", \"dataset_id\", \"source_id\", \"leaf_node\")\n",
    "    .unique(subset=[\"dataset_id\",\"leaf_node\",\"source_id\"])\n",
    "    .collect(streaming=True)\n",
    "    )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (0, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>dataset_id</th><th>source_id</th><th>plate_id</th><th>load_data_csv_root_dir</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (0, 4)\n",
       "┌────────────┬───────────┬──────────┬────────────────────────┐\n",
       "│ dataset_id ┆ source_id ┆ plate_id ┆ load_data_csv_root_dir │\n",
       "│ ---        ┆ ---       ┆ ---      ┆ ---                    │\n",
       "│ str        ┆ str       ┆ str      ┆ str                    │\n",
       "╞════════════╪═══════════╪══════════╪════════════════════════╡\n",
       "└────────────┴───────────┴──────────┴────────────────────────┘"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#24 plates of the source 15 in JUMP have a load_data.csv without having a load_data_csv_root_dir\n",
    "# also, the dataset_id is 'jump' and not 'cpg0016-jump'\n",
    "df = (index\n",
    "      .unique(subset=[\"dataset_id\",\"plate_id\"])\n",
    "      .filter(pl.col(\"leaf_node\").eq(\"load_data.csv\"))\n",
    "      .select(pl.col([\"dataset_id\",\"source_id\",\"plate_id\",\"load_data_csv_root_dir\"]))\n",
    "      .filter(pl.col(\"load_data_csv_root_dir\").is_null())  #https://docs.pola.rs/py-polars/html/reference/dataframe/api/polars.DataFrame.drop_nulls.html\n",
    "      .collect(streaming=True)\n",
    "      )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (460, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>plate_id</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;PEC00001815__2022-02-23T18_03_17-Measurement3&quot;</td></tr><tr><td>&quot;PEP00004102__2021-12-02T12_05_47-Measurement1&quot;</td></tr><tr><td>&quot;PEP00004137__2021-12-01T16_50_57-Measurement1&quot;</td></tr><tr><td>&quot;PEP00004139__2021-12-03T05_02_12-Measurement1&quot;</td></tr><tr><td>&quot;PEC00001790&quot;</td></tr><tr><td>&quot;PEP00004092&quot;</td></tr><tr><td>&quot;PEP00004102&quot;</td></tr><tr><td>&quot;PEC00001785__2021-12-07T01_54_26-Measurement1&quot;</td></tr><tr><td>&quot;PEP00004041__2021-12-16T04_45_56-Measurement1&quot;</td></tr><tr><td>&quot;PEC00001854__2021-12-17T13_35_15-Measurement1&quot;</td></tr><tr><td>&quot;PEP00004072&quot;</td></tr><tr><td>&quot;PEP00004065__2021-12-13T12_34_12-Measurement1&quot;</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;PEC00001842__2021-12-17T04_56_37-Measurement1&quot;</td></tr><tr><td>&quot;PEP00004049__2022-02-22T10_42_29-Measurement1&quot;</td></tr><tr><td>&quot;PEC00001837&quot;</td></tr><tr><td>&quot;PEP00004026__2021-12-15T18_17_27-Measurement1&quot;</td></tr><tr><td>&quot;PEP00004023__2022-02-23T16_16_10-Measurement2&quot;</td></tr><tr><td>&quot;PEC00001858__2021-12-17T10_07_06-Measurement1&quot;</td></tr><tr><td>&quot;PEC00001863__2022-01-18T15_18_04-Measurement1&quot;</td></tr><tr><td>&quot;PEC00001805__2022-02-24T19_35_58-Measurement2&quot;</td></tr><tr><td>&quot;PEP00004326__2022-03-01T06_12_25-Measurement1&quot;</td></tr><tr><td>&quot;PEC00001835&quot;</td></tr><tr><td>null</td></tr><tr><td>&quot;PEP00004136&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (460, 1)\n",
       "┌───────────────────────────────────────────────┐\n",
       "│ plate_id                                      │\n",
       "│ ---                                           │\n",
       "│ str                                           │\n",
       "╞═══════════════════════════════════════════════╡\n",
       "│ PEC00001815__2022-02-23T18_03_17-Measurement3 │\n",
       "│ PEP00004102__2021-12-02T12_05_47-Measurement1 │\n",
       "│ PEP00004137__2021-12-01T16_50_57-Measurement1 │\n",
       "│ PEP00004139__2021-12-03T05_02_12-Measurement1 │\n",
       "│ …                                             │\n",
       "│ PEP00004326__2022-03-01T06_12_25-Measurement1 │\n",
       "│ PEC00001835                                   │\n",
       "│ null                                          │\n",
       "│ PEP00004136                                   │\n",
       "└───────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 460 plate_ids for source 15 in JUMP, are there really 460 plates? \n",
    "# also, plate_id varies in structure!\n",
    "df = (index\n",
    "      .filter(pl.col(\"dataset_id\").eq(\"jump\"))\n",
    "      .filter(pl.col(\"source_id\").eq(\"source_15\"))\n",
    "      .unique(subset=[\"plate_id\"])\n",
    "      .select(pl.col([\"plate_id\"]))\n",
    "      .collect(streaming=True)\n",
    "      )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (183, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>plate_id</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;PEC00001782&quot;</td></tr><tr><td>&quot;PEC00001783&quot;</td></tr><tr><td>&quot;PEC00001784&quot;</td></tr><tr><td>&quot;PEC00001785&quot;</td></tr><tr><td>&quot;PEC00001786&quot;</td></tr><tr><td>&quot;PEC00001787&quot;</td></tr><tr><td>&quot;PEC00001788&quot;</td></tr><tr><td>&quot;PEC00001789&quot;</td></tr><tr><td>&quot;PEC00001790&quot;</td></tr><tr><td>&quot;PEC00001791&quot;</td></tr><tr><td>&quot;PEC00001792&quot;</td></tr><tr><td>&quot;PEC00001793&quot;</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;PEP00004335&quot;</td></tr><tr><td>&quot;PEP00004421&quot;</td></tr><tr><td>&quot;PEP00004422&quot;</td></tr><tr><td>&quot;PEP00004423&quot;</td></tr><tr><td>&quot;PEP00004425&quot;</td></tr><tr><td>&quot;PEP00004426&quot;</td></tr><tr><td>&quot;PEP00004427&quot;</td></tr><tr><td>&quot;PEP00004430&quot;</td></tr><tr><td>&quot;PEP00004431&quot;</td></tr><tr><td>&quot;PEP00004432&quot;</td></tr><tr><td>&quot;PEP00004457&quot;</td></tr><tr><td>&quot;PEP00004458&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (183, 1)\n",
       "┌─────────────┐\n",
       "│ plate_id    │\n",
       "│ ---         │\n",
       "│ str         │\n",
       "╞═════════════╡\n",
       "│ PEC00001782 │\n",
       "│ PEC00001783 │\n",
       "│ PEC00001784 │\n",
       "│ PEC00001785 │\n",
       "│ …           │\n",
       "│ PEP00004431 │\n",
       "│ PEP00004432 │\n",
       "│ PEP00004457 │\n",
       "│ PEP00004458 │\n",
       "└─────────────┘"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For source 15 in JUMP, are there really 460 plates? \n",
    "# There are only 183 unique ones matching the regex for the plate name structure\n",
    "df = (index\n",
    "      .filter(pl.col(\"dataset_id\").eq(\"jump\"))\n",
    "      .filter(pl.col(\"source_id\").eq(\"source_15\"))\n",
    "      # .unique(subset=[\"plate_id\"])\n",
    "      .filter(pl.col(\"plate_id\").str.contains(\"^PE(P|C)[0-9]{8}$\"))\n",
    "      .select(pl.col([\"plate_id\"]).unique().sort())\n",
    "      .collect(streaming=True)\n",
    "      )\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
